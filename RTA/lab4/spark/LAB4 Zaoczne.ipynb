{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12259b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# na te zajęcia potrzebować będziemy wersji lokalnej SPARKA (może być docker !)\n",
    "# git clone \n",
    "# docker build -t spark .\n",
    "# docker run -p 8888:8888 spark\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread\n",
    "# and batch interval of 1 second\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\") \\\n",
    "        .appName(\"Stream_Socket\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1694ba83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# biblioteka re - regular expression \n",
    "import re\n",
    "# =================\n",
    "# RDD - czyli podstawowy obiekt do obsługi danych dla SPARKA\n",
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691c7a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 12),\n",
       " ('engine', 1),\n",
       " ('compatible', 1),\n",
       " ('hadoop', 3),\n",
       " ('run', 1),\n",
       " ('in', 3),\n",
       " ('clusters', 1),\n",
       " ('yarn', 1),\n",
       " (\"spark's\", 1),\n",
       " ('mode', 1),\n",
       " ('process', 1),\n",
       " ('hdfs', 1),\n",
       " ('cassandra', 1),\n",
       " ('hive', 1),\n",
       " ('designed', 1),\n",
       " ('perform', 1),\n",
       " ('both', 1),\n",
       " ('similar', 1),\n",
       " ('new', 1),\n",
       " ('like', 1),\n",
       " ('streaming', 1),\n",
       " ('machine', 1),\n",
       " ('learning', 1),\n",
       " ('than', 8),\n",
       " ('implicit', 1),\n",
       " ('simple', 1),\n",
       " ('complicated', 1),\n",
       " ('flat', 1),\n",
       " ('nested', 1),\n",
       " ('sparse', 1),\n",
       " ('readability', 1),\n",
       " ('counts', 1),\n",
       " ('cases', 1),\n",
       " (\"aren't\", 1),\n",
       " ('rules', 1),\n",
       " ('although', 3),\n",
       " ('errors', 1),\n",
       " ('never', 3),\n",
       " ('pass', 1),\n",
       " ('explicitly', 1),\n",
       " ('silenced', 1),\n",
       " ('of', 2),\n",
       " ('refuse', 1),\n",
       " ('temptation', 1),\n",
       " ('guess', 1),\n",
       " ('there', 1),\n",
       " ('only', 1),\n",
       " ('way', 2),\n",
       " ('do', 2),\n",
       " ('may', 2),\n",
       " ('at', 1),\n",
       " (\"you're\", 1),\n",
       " ('dutch', 1),\n",
       " ('now', 2),\n",
       " ('right', 1),\n",
       " ('idea', 3),\n",
       " ('good', 1),\n",
       " ('are', 1),\n",
       " ('honking', 1),\n",
       " (\"let's\", 1),\n",
       " ('more', 1),\n",
       " ('spark', 1),\n",
       " ('a', 3),\n",
       " ('fast', 1),\n",
       " ('and', 6),\n",
       " ('general', 1),\n",
       " ('processing', 2),\n",
       " ('with', 1),\n",
       " ('data', 2),\n",
       " ('it', 5),\n",
       " ('can', 2),\n",
       " ('through', 1),\n",
       " ('or', 1),\n",
       " ('standalone', 1),\n",
       " ('hbase', 1),\n",
       " ('any', 1),\n",
       " ('inputformat', 1),\n",
       " ('to', 7),\n",
       " ('batch', 1),\n",
       " ('mapreduce', 1),\n",
       " ('workloads', 1),\n",
       " ('interactive', 1),\n",
       " ('queries', 1),\n",
       " ('beautiful', 1),\n",
       " ('better', 8),\n",
       " ('ugly', 1),\n",
       " ('explicit', 1),\n",
       " ('complex', 2),\n",
       " ('dense', 1),\n",
       " ('special', 2),\n",
       " ('enough', 1),\n",
       " ('break', 1),\n",
       " ('the', 5),\n",
       " ('practicality', 1),\n",
       " ('beats', 1),\n",
       " ('purity', 1),\n",
       " ('should', 2),\n",
       " ('silently', 1),\n",
       " ('unless', 2),\n",
       " ('face', 1),\n",
       " ('ambiguity', 1),\n",
       " ('be', 3),\n",
       " ('one', 3),\n",
       " ('preferably', 1),\n",
       " ('obvious', 2),\n",
       " ('that', 1),\n",
       " ('not', 1),\n",
       " ('first', 1),\n",
       " ('often', 1),\n",
       " ('if', 2),\n",
       " ('implementation', 2),\n",
       " ('hard', 1),\n",
       " ('explain', 2),\n",
       " (\"it's\", 1),\n",
       " ('bad', 1),\n",
       " ('easy', 1),\n",
       " ('namespaces', 1),\n",
       " ('great', 1),\n",
       " ('those', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================\n",
    "# Prosty Word Count \n",
    "# r'[a-z'] - oznacza jeden znak dla liter od a do z (male) + apostrof\n",
    "# []+ - oznacza, że może pojawić się jeden lub więcej znaków\n",
    "# pamiętajcie, że przetwarzamy x.lower()\n",
    "# Licznik ten jest wygenerowany na podstawie map-reduce\n",
    "# ===================\n",
    "\n",
    "sc.textFile(\"RDD_input\") \\\n",
    ".map(lambda x: re.findall(r\"[a-z']+\", x.lower())) \\\n",
    ".flatMap(lambda x: [(y, 1) for y in x]) \\\n",
    ".reduceByKey(lambda x,y: x + y) \\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871fec78",
   "metadata": {},
   "source": [
    "## SPARK STREAMING\n",
    "\n",
    "Część Sparka odpowiedzialna za przetwarzanie danych w czasie rzeczywistym. \n",
    "\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-arch.png\"/>\n",
    "\n",
    "Dane mogą pochodzić z różnych źródeł np. sokety TCP, Kafka, etc. \n",
    "Korzystając z poznanych już metod `map, reduce, join, oraz window` można w łatwy sposób generować przetwarzanie strumienia tak jaby był to nieskończony ciąg RDD. \n",
    "Ponadto nie ma problemu aby wywołać na strumieniu operacje ML czy wykresy. \n",
    "\n",
    "Cała procedura przedstawia się następująco: \n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-flow.png\"/>\n",
    "\n",
    "SPARK STREAMING w tej wersji wprowadza abstrakcje zwaną `discretized stream` *DStream* (reprezentuje sekwencję RDD).\n",
    "\n",
    "Operacje na DStream można wykonywać w API JAVA, SCALA, Python, R (nie wszystkie możliwości są dostępne dla Pythona). \n",
    "\n",
    "## Spark Streaming potrzebuje minium 2 rdzenie !\n",
    "\n",
    "----\n",
    "- **StreamingContext(sparkContext, batchDuration)** - reprezentuje połączenie z klastrem i służy do tworzenia DStreamów, `batchDuration` wskazuje na granularność batch'y (w sekundach)\n",
    "- **socketTextStream(hostname, port)** - tworzy DStream na podstawie danych napływających ze wskazanego źródła TCP\n",
    "- **flatMap(f), map(f), reduceByKey(f)** - działają analogicznie jak w przypadku RDD z tym że tworzą nowe DStream'y\n",
    "- **pprint(n)** - printuje pierwsze `n` (domyślnie 10) elementów z każdego RDD wygenerowanego w DStream'ie\n",
    "- **StreamingContext.start()** - rozpoczyna działania na strumieniach\n",
    "- **StreamingContext.awaitTermination(timeout)** - oczekuje na zakończenie działań na strumieniach\n",
    "- **StreamingContext.stop(stopSparkContext, stopGraceFully)** - kończy działania na strumieniach\n",
    "\n",
    "Obiekt StreamingContext można wygenerować za pomocą obiektu SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4415d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 5) # ustawiłem na 5 sekund aby był czas na wpisywanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2697f3f",
   "metadata": {},
   "source": [
    "Po wygenerowaniu obiektu `ssc` musisz wskazać źródło i utworzyć na jego podstawie DStream. Określić wszystkie transformacje. Uruchomić metodę `start()`, która powoduje nasłuchiwanie. Włączyć oczekiwanie na zakończenie procesu `awaitTermination()` bądź zatrzymać nasłuch ręcznie `stop()`. \n",
    "\n",
    "- po rozpoczęciu nasłuchu nie można już ustawić nowych przekształceń !\n",
    "- po zatrzymaniu nie można zrestartować\n",
    "- tylko jeden StreamingContext aktywny na JVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afee1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DStream - dane pobierane z socketu TCP - na porcie 9999\n",
    "\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da08c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "wordCounts = words.map(lambda word: (word,1)).reduceByKey(lambda x,y: x+y)\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82941575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:01:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:01:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:01:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9bdaa431be11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# Start the computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for the computation to terminate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate\n",
    "ssc.stop(True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50fe0bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: Nmap: not found\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:16:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:16:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:16:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:16:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:16:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:16:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:17:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:17:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:17:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:17:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:17:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:17:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:17:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:17:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:17:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-06-03 10:17:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pierwsza wersja nadawania na TCP 9999 (unix)\n",
    "# w konsoli linuxowej netcat Nmap for windows\n",
    "!Nmap -lk 9999\n",
    "\n",
    "# wpisujesz tekst\n",
    "# jeśli się nie uda to generuj plik poniżej (Możecie też zmienić plik źródłowy)\n",
    "# zweryfikujcie żeby ścieżka była odpowiednia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28565fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file stream.py\n",
    "\n",
    "from socket import *\n",
    "import time\n",
    "\n",
    "rdd = list()\n",
    "with open(\"RDD_input\", 'r') as ad:\n",
    "    for line in ad:\n",
    "        rdd.append(line)\n",
    "\n",
    "HOST = 'localhost'\n",
    "PORT = 9999\n",
    "ADDR = (HOST, PORT)\n",
    "tcpSock = socket(AF_INET, SOCK_STREAM)\n",
    "tcpSock.bind(ADDR)\n",
    "tcpSock.listen(5)\n",
    "\n",
    "\n",
    "while True:\n",
    "    c, addr = tcpSock.accept()\n",
    "    print('got connection')\n",
    "    for line in rdd:\n",
    "        try:\n",
    "            c.send(line.encode())\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            break\n",
    "    c.close()\n",
    "    print('disconnected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w osobnej konsoli (możesz uruchomić ją w jupyter notebook)\n",
    "! python stream.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e879cf8",
   "metadata": {},
   "source": [
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream.png\"/>\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac032a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dane do strumienia można wygenerować też jako listę i wykorzystać tzw kolejkę \n",
    "## kolejka to tak jak w sklepie obsługujemy pierwszego (ostatni dochodzi na koniec kolejki)\n",
    "## po obsłużeniu wypada pierwszy i kolejka się zmniejsza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c35b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\") \\ # TUTAJ WAŻNE ABY BYŁY PRZYNAJMNIEJ DWA PROCESORY\n",
    "        .appName(\"Stream_Kolejka\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1) # ustawiłem na 5 sekund aby był czas na wpisywanie\n",
    "\n",
    "# 10 pakietów po 1000 liczb\n",
    "rddQueue = []\n",
    "for i in range(10):\n",
    "        rddQueue += [sc.parallelize(\n",
    "            [j for j in range(1, 1001)], 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputStream = ssc.queueStream(rddQueue)\n",
    "\n",
    "mappedStream = inputStream.map(lambda x: (x % 10, 1))\n",
    "reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "reducedStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ad5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tym razem trzeba trochę inaczej uruchomić i wprowadzić czas między\n",
    "import time\n",
    "ssc.start()\n",
    "time.sleep(10)\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d19cc6",
   "metadata": {},
   "source": [
    "## Stateful Wordcount \n",
    "\n",
    "Operacja `updateStateByKey` pozwala łączyć ze sobą wyniki otrzymywane na poszczególbych DStreamach. Dzięki tej operacji możesz w sposób ciągły uzupełniać informacje !\n",
    "\n",
    "Aby Spark Streaming mógł łączyć dane z wielu batchy (stateful transformations) konieczne jest wskazanie lokalizacji gdzie zapisywane będą checkpointy.\n",
    "\n",
    "1. Zdefiniuj stan podstawowy\n",
    "2. wskaż funkcję łączącą \n",
    "\n",
    "----\n",
    "- **checkpoint(directory)** - wskazuje gdzie zapisywane będą checkpointy z operacji na DStream'ach\n",
    "- **updateStateByKey(updateFunc)** - zwraca nowy DStream zawierający informację o bieżącym stanie poszczególnych kluczy, stan każdego klucza odświeżany jest przy pomocy `updateFunc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFunc(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\") \\ # TUTAJ WAŻNE ABY BYŁY PRZYNAJMNIEJ DWA PROCESORY\n",
    "        .appName(\"Stream_stateful\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutaj zdecydujcie czy puszczacie z pliku czy z konsoli (z konsoli dobrze widac jak dziala)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "runningCounts = pairs.updateStateByKey(updateFunc)\n",
    "\n",
    "runningCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5425302",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n",
    "ssc.stop(True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8999cd8",
   "metadata": {},
   "source": [
    "## Redukcja w oknach \n",
    "\n",
    "----\n",
    "- **reduceByKeyAndWindow(func, invFunc, windowDuration, slideDuration)** - zwraca nowy DStream powstały w wyniku stosowania przyrostowo reduceByKey wewnątrz zdefiniowanego okna. Zredukowane wartości dla nowego okna obliczane są z wykorzystaniem wartości starego okna poprzez: \n",
    "1. zredukowanie (dodanie) nowych wartości, \n",
    "2. \"odwrotne zredukowanie\" (odjęcie) wartości które opuściły już okno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148714ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\") \\ # TUTAJ WAŻNE ABY BYŁY PRZYNAJMNIEJ DWA PROCESORY\n",
    "        .appName(\"Stream_windows\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 2)\n",
    "ssc.checkpoint(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "pairs = words.map(lambda word: (word,1))\n",
    "# window length - długość trwania okna\n",
    "# sliding interval - czas w którym wykonywana jest funkcja okna \n",
    "windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)\n",
    "\n",
    "windowedWordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "ssc.stop(True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29fd5d6",
   "metadata": {},
   "source": [
    "### Stream do DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab1c2c3",
   "metadata": {},
   "source": [
    "lines = DataFrame reprezentujący nieograniczoną tabelę zawierającą dane strumieniowe. \n",
    "Zawiera ona jedną kolumnę o nazwie `value`. Każda nowa linia to wiersz w tabeli. \n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b608a",
   "metadata": {},
   "source": [
    "## SPARK Structured Streaming\n",
    "\n",
    "----\n",
    "- **SparkSession.readStream.format(source).option(key, value).load()** - tworzy strumieniowy DataFrame\n",
    "- **DataFrame.writeStream.outputMode(mode).format(source).option(key, value).start()** - wysyła dane ze strumieniowego DataFrame'u \"na zewnątrz\"; `complete` mode - outputem jest cała zaktualizowana tabela, `append` mode - outputem są jedynie nowe wiersze, `update` mode - outputem są jedynie zaktualizowane wiersze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48925033",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file spark2.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\") \\\n",
    "        .appName(\"Stream_DF\")\\\n",
    "        .getOrCreate()\n",
    "    print(\"=\"*50)\n",
    "    print(\"Zaczynamy DataFrame\")\n",
    "    print(\"=\"*50)\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    lines = spark.readStream\\\n",
    "        .format(\"socket\")\\\n",
    "        .option(\"host\", \"localhost\")\\\n",
    "        .option(\"port\", 9999)\\\n",
    "        .load()\n",
    "    words = lines.select(f.explode(f.split(lines.value, \" \")).alias(\"word\"))\n",
    "    wordCounts = words.groupBy(\"word\").count()\n",
    "    query = wordCounts.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "    query.awaitTermination()\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b20a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "! spark-submit spark2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
